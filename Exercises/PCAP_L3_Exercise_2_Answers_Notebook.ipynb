{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PCAP L3 â€” Exercise 2 (Answers Notebook)\n",
        "\n",
        "`Mission 02`: Clean, classify, and filter user input using string operations.\n",
        "\n",
        "**Context:** You're building a tiny input-cleanup and classification helper for a signup system. Users type messy usernames and random tokens from time-to-time, so we need a system to prevent this from affecting what our system works with.\n",
        "\n",
        "Your job is to:\n",
        "- Normalize usernames into a safe internal format,\n",
        "- Classify tokens as alphabetic, numeric, alphanumeric, or â€œotherâ€, and\n",
        "- Filter lists of words by a prefix (for quick search / autocomplete).\n",
        "\n",
        "> ðŸŽ¯ Your Key Objectives:\n",
        "> - Practice `strip()`, `lower()`, and `replace()` for normalization.\n",
        "> - Use `isalpha()`, `isdigit()`, and `isalnum()` for classification.\n",
        "> - Use `startswith()` and simple loops to filter strings."
      ],
      "metadata": {
        "id": "5pTpMRtMT8Gy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Username Normalizer\n",
        "\n",
        "We want to turn messy usernames like `\"  Alice Smith  \"` into a consistent internal format:\n",
        "- Trim leading/trailing whitespace\n",
        "- Convert everything to lowercase\n",
        "- Replace spaces with underscores\n",
        "\n",
        "Your Tasks:\n",
        "- [ ] 1.1 - Write a `normalize_username()` function that takes a single parameter `raw_name`. Use a type annotation to indicate that `raw_name` is a string, and add a return type annotation showing that the function returns a string as well.\n",
        "- [ ] 1.2 - Use `.strip()` to remove leading/trailing whitespace.\n",
        "- [ ] 1.3 - Convert to lowercase with `.lower()`.\n",
        "- [ ] 1.4 - Replace internal spaces `\" \"` with `\"_\"` using `.replace()`.\n",
        "- [ ] 1.5 - Return the cleaned username string.\n"
      ],
      "metadata": {
        "id": "bSs27PRsvjnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Part 1: Your code here ===\n",
        "\n",
        "# 1.1)\n",
        "def normalize_username(raw_name: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalize a raw username string by:\n",
        "      - stripping whitespace\n",
        "      - converting to lowercase\n",
        "      - replacing spaces with underscores\n",
        "    \"\"\"\n",
        "    # 1.2)\n",
        "    cleaned = raw_name.strip()\n",
        "    # 1.3)\n",
        "    cleaned = cleaned.lower()\n",
        "    # 1.4)\n",
        "    cleaned = cleaned.replace(\" \", \"_\")\n",
        "    # 1.5)\n",
        "    return cleaned"
      ],
      "metadata": {
        "id": "AG9f3xY01fXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Token Classifier (`isalpha`, `isdigit`, `isalnum`)\n",
        "\n",
        "Now we'll classify a single token into a small set of categories:\n",
        "\n",
        "- `\"alpha\"`        â†’ only letters (e.g. `\"Hello\"`)\n",
        "- `\"numeric\"`      â†’ only digits (e.g. `\"123\"`)\n",
        "- `\"alphanumeric\"` â†’ mixture of letters and digits (e.g. `\"user123\"`)\n",
        "- `\"other\"`        â†’ anything else (symbols, spaces, punctuation, etc.)\n",
        "\n",
        "Your Tasks:\n",
        "- [ ] 2.1 - Write a `classify_token()` function that takes `token`. Use a type annotation to indicate that `token` is a string, and add a return type annotation showing that the function returns a string as well.\n",
        "- [ ] 2.2 - If `token.isalpha()` is `True`, return `\"alpha\"`.\n",
        "- [ ] 2.3 - `elif token.isdigit()` is `True`, return `\"numeric\"`.\n",
        "- [ ] 2.4 - `elif token.isalnum()` is `True`, return `\"alphanumeric\"`.\n",
        "- [ ] 2.5 - Otherwise, return `\"other\"`.\n"
      ],
      "metadata": {
        "id": "gfQFPdVd14tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Part 2: Your code here ===\n",
        "\n",
        "# 2.1)\n",
        "def classify_token(token: str) -> str:\n",
        "    \"\"\"\n",
        "    Classify a token as: 'alpha', 'numeric', 'alphanumeric', or 'other'.\n",
        "    \"\"\"\n",
        "    # 2.2)\n",
        "    if token.isalpha():\n",
        "        return \"alpha\"\n",
        "    # 2.3)\n",
        "    elif token.isdigit():\n",
        "        return \"numeric\"\n",
        "    # 2.4)\n",
        "    elif token.isalnum():\n",
        "        return \"alphanumeric\"\n",
        "    # 2.5)\n",
        "    else:\n",
        "        return \"other\"\n"
      ],
      "metadata": {
        "id": "PfE22U7P2T17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Prefix Filter (Case-Insensitive)\n",
        "\n",
        "We want to filter a list of words using a prefix, for example:\n",
        "\n",
        "- words: `[\"alpha\", \"Alpine\", \"beta\", \"ALGORITHM\"]`\n",
        "- prefix: `\"al\"`\n",
        "- result: `[\"alpha\", \"Alpine\", \"ALGORITHM\"]`\n",
        "  - Note: this is a case-insensitive match\n",
        "\n",
        "Your Tasks:\n",
        "- [ ] 3.1 - Write a `filter_by_prefix()` function that takes `words` (a list of strings) and `prefix` (a string), and add a return type annotation showing that the function returns a list of strings.\n",
        "- [ ] 3.2 - Convert `prefix` to lowercase once at the start.\n",
        "- [ ] 3.3 - Create an empty list `result`.\n",
        "- [ ] 3.4 - Loop over each `word` in `words`.\n",
        "- [ ] 3.5 - For each word, compare `word.lower().startswith(lower_prefix)`.\n",
        "- [ ] 3.6 - If it matches, append `word` to `result`.\n",
        "- [ ] 3.7 - Return `result`.\n"
      ],
      "metadata": {
        "id": "-UyUNOXd4djc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Part 3: Your code here ===\n",
        "\n",
        "# 3.1)\n",
        "def filter_by_prefix(words: list[str], prefix: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Return a new list containing only words that start with the given prefix\n",
        "    (case-insensitive).\n",
        "    \"\"\"\n",
        "    # 3.2)\n",
        "    lower_prefix = prefix.lower()\n",
        "    # 3.3)\n",
        "    result: list[str] = []\n",
        "    # 3.4)\n",
        "    for word in words:\n",
        "        # 3.5)\n",
        "        if word.lower().startswith(lower_prefix):\n",
        "            # 3.6)\n",
        "            result.append(word)\n",
        "    # 3.7)\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "ZkCVvjRD5bEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Mission Run â€” Exercise All Paths\n",
        "\n",
        "Context: Once you've filled in Parts 1-3, run the tests below without changing them. They should help you confirm your functions work for different inputs."
      ],
      "metadata": {
        "id": "VnT3_FWaWsBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Tests for Part 1: normalize_username() ===\n",
        "\n",
        "print(normalize_username(\"  Alice Smith  \"))   # Expected: alice_smith\n",
        "print(normalize_username(\"BOB\"))               # Expected: bob\n",
        "print(normalize_username(\"  charlie  brown\"))  # Expected: charlie__brown\n",
        "print(normalize_username(\"  data  wizard \"))   # Expected: data__wizard"
      ],
      "metadata": {
        "id": "wxvLWUjpWrZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c6fce8f-da40-4df2-be11-c4cdccc5e40c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alice_smith\n",
            "bob\n",
            "charlie__brown\n",
            "data__wizard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Tests for Part 2: classify_token() ===\n",
        "\n",
        "tokens = [\"Hello\", \"123\", \"user007\", \"hi!\", \"42.0\", \"\"]\n",
        "\n",
        "for t in tokens:\n",
        "    print(f\"{t!r:8} -> {classify_token(t)}\")\n",
        "\n",
        "# Expected:\n",
        "# 'Hello'   -> alpha\n",
        "# '123'     -> numeric\n",
        "# 'user007' -> alphanumeric\n",
        "# 'hi!'     -> other\n",
        "# '42.0'    -> other\n",
        "# ''        -> other (empty string is not alpha/digit/alnum)"
      ],
      "metadata": {
        "id": "axHoI3BvXNct",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "493120e3-0e9c-45b5-bbb3-5268db865af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Hello'  -> alpha\n",
            "'123'    -> numeric\n",
            "'user007' -> alphanumeric\n",
            "'hi!'    -> other\n",
            "'42.0'   -> other\n",
            "''       -> other\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Tests for Part 3: filter_by_prefix() ===\n",
        "\n",
        "words = [\"alpha\", \"Alpine\", \"beta\", \"ALGORITHM\", \"test\", \"align\", \"Gamma\"]\n",
        "\n",
        "print(filter_by_prefix(words, \"al\"))   # Expected: ['alpha', 'Alpine', 'ALGORITHM', 'align']\n",
        "print(filter_by_prefix(words, \"AL\"))   # Expected: ['alpha', 'Alpine', 'ALGORITHM', 'align']\n",
        "print(filter_by_prefix(words, \"ga\"))   # Expected: ['Gamma']\n",
        "print(filter_by_prefix(words, \"z\"))    # Expected: []"
      ],
      "metadata": {
        "id": "OGG2eAb8e6io",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f17ad40-56a7-44b9-d3e7-001d04db33bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['alpha', 'Alpine', 'ALGORITHM', 'align']\n",
            "['alpha', 'Alpine', 'ALGORITHM', 'align']\n",
            "['Gamma']\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Debrief (short answers)"
      ],
      "metadata": {
        "id": "NRREkt0yXR2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1 of 2: Why might a system want to normalize usernames before storing them in a database?"
      ],
      "metadata": {
        "id": "8-exHh7FXWLX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization makes it easier to:\n",
        "- avoid duplicate accounts that differ only by case (i.e. `A` vs. `a`) or spaces,\n",
        "- compare usernames reliably, or\n",
        "- enforce a simple, safe, internal format.\n",
        "\n",
        "It also reduces bugs you face when searching, sorting, or checking if a username is already in use.\n",
        "\n",
        "_______"
      ],
      "metadata": {
        "id": "IfwvdgBbXYim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2 of 2: When would you prefer a 'case-insensitive' prefix search over a 'case-sensitive' one?\n"
      ],
      "metadata": {
        "id": "DgGg03Z2XZgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Case-insensitive prefix searching is useful when user input might vary in capitalization, for example, in:\n",
        "- search bars,\n",
        "- autocomplete,\n",
        "- command palettes, and\n",
        "- filtering user lists.\n",
        "\n",
        "Basically, you usually care about the letters, not how the user capitalized them.  \n",
        "\n",
        "Case-sensitive search is more appropriate when the exact casing carries meaning, for example:\n",
        "- passwords,\n",
        "- codes, or\n",
        "- identifiers."
      ],
      "metadata": {
        "id": "VmU7MJ1hkk3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Your answers here:*\n",
        "\n",
        "_______"
      ],
      "metadata": {
        "id": "rXwzrSuXYJ73"
      }
    }
  ]
}